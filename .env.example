# BiTranslator environment variable configuration
# Copy this file to .env and fill in your configuration

# LLM provider: "openai", "gemini", or "ollama"
BT_LLM_PROVIDER=gemini

# API Key (Gemini, OpenAI, or compatible API)
# For Gemini, you can also set GEMINI_API_KEY env var instead (auto-discovered by SDK)
BT_LLM_API_KEY=your-gemini-api-key-here

# API Base URL (only needed for OpenAI-compatible providers, not for Gemini)
# OpenAI: https://api.openai.com/v1
# DeepSeek: https://api.deepseek.com/v1
# Local Ollama: http://localhost:11434/v1
BT_LLM_BASE_URL=

# Model for analysis
BT_LLM_MODEL=gemini-2.5-pro

# Model for translation (leave empty to use analysis model)
BT_TRANSLATION_MODEL=

# Temperature (0-2, 0.3 recommended for translation)
BT_LLM_TEMPERATURE=0.3

# Max output tokens for analysis/strategy calls
BT_LLM_MAX_TOKENS=8192

# Max output tokens for translation (Gemini supports up to 65536)
BT_LLM_TRANSLATION_MAX_TOKENS=65536

# Maximum characters per translation chunk (smaller = safer, but more API calls)
BT_MAX_CHAPTER_CHARS=6000

# Max auto-continuation attempts when translation is truncated
BT_MAX_CONTINUATIONS=5
